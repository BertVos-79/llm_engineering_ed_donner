{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be5ab64-6cfc-437a-8e24-26535a08312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcab1473-498b-4aa4-8737-136036ae753c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key and api_key.startswith(\"sk-proj-\") and len(api_key) > 10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae99f068-d9ab-45a0-86d4-eb2933769815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI instance\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b02a071-8adb-4c93-95f5-e2892df3c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutor system prompt\n",
    "tutor_system_prompt = \"\"\"Role and Purpose: You are a tutor for the \"Large Language Model Engineering\" \n",
    "course by Ed Donner. Your role is to provide structured, clear, and patient explanations, helping \n",
    "learners understand concepts and apply them effectively.\n",
    "\n",
    "Start with context, progressing from basic overviews to detailed insights.\n",
    "Use concise language, real-world examples, and analogies to simplify complex ideas.\n",
    "Summarize key points, invite further questions and suggest further resources for exploration.\n",
    "\n",
    "Tone:\n",
    "Maintain professionalism, avoid assumptions about learners' backgrounds, and focus strictly\n",
    "on course-relevant content. Use examples and citations aligned with the course material.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf0295c-e1cf-44e0-a465-7e687d8717d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for OpenAI GPT response (streaming)\n",
    "def stream_gpt(message):\n",
    "    tutor_messages = [\n",
    "        {\"role\": \"system\", \"content\": tutor_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=tutor_messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    explanation = \"\"\n",
    "    for chunk in stream:\n",
    "        explanation += chunk.choices[0].delta.content or \"\"\n",
    "        yield explanation.replace(\"```\", \"\").replace(\"markdown\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc58694-54eb-473c-9b5b-ad0a17654e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Llama response\n",
    "def explain_with_llama(message):\n",
    "    tutor_messages = [\n",
    "        {\"role\": \"system\", \"content\": tutor_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    response = ollama.chat(model=MODEL_LLAMA, messages=tutor_messages)\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0048cf57-e327-482c-afdf-53082e207921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Tool for fetching webpage content\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535d98c1-85b8-4883-a5ef-244d7a42770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified streaming function for Gradio\n",
    "def stream_explanation(question, model):\n",
    "    if model == \"GPT\":\n",
    "        yield from stream_gpt(question)\n",
    "    elif model == \"Llama\":\n",
    "        explanation = explain_with_llama(question)\n",
    "        yield explanation\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4b0c6a0-0842-4c22-b207-5f4e11fc5d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio Interface with Markdown rendering for GPT\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        question_input = gr.Textbox(label=\"Enter your question:\", placeholder=\"Type your technical question here\")\n",
    "        model_selector = gr.Dropdown([\"GPT\", \"Llama\"], label=\"Select model\", value=\"GPT\")\n",
    "    with gr.Row():\n",
    "        output_area = gr.Markdown(label=\"Response:\")\n",
    "    with gr.Row():\n",
    "        clear_button = gr.Button(\"Clear\")\n",
    "\n",
    "    def handle_question(question, model):\n",
    "        if model == \"GPT\":\n",
    "            for chunk in stream_gpt(question):  # Stream GPT response\n",
    "                yield chunk  # Only yield the new chunk\n",
    "        elif model == \"Llama\":\n",
    "            yield explain_with_llama(question)  # Provide Llama response\n",
    "        else:\n",
    "            yield \"Unknown model selected.\"\n",
    "\n",
    "    question_input.submit(handle_question, inputs=[question_input, model_selector], outputs=output_area)\n",
    "    clear_button.click(lambda: \"\", None, output_area)\n",
    "\n",
    "ui.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155aa94-14da-45cf-a476-cd168a494a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
