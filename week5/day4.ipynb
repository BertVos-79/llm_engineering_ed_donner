{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "# RAG part 4\n",
    "\n",
    "### Integrating RAG into LLM\n",
    "\n",
    "### Expert Knowledge Worker\n",
    "\n",
    "A question answering agent that is an expert knowledge worker\n",
    "To be used by employees of Insurellm, an Insurance Tech company\n",
    "The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c9dc2-285b-4a2f-ba10-eaba0efdbd52",
   "metadata": {},
   "source": [
    "## 1. Load Data with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da54b1-597a-443a-a9de-694361ebe214",
   "metadata": {},
   "source": [
    "## 2. Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd06e02f-6d9b-44cc-a43d-e1faa8acc7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c54b4b6-06da-463d-bee7-4dd456c2b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: contracts, company, employees, products\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1460b-192f-472e-b0e4-a79185bd3e5d",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad798040-672a-4dbc-8406-c83606cad6ed",
   "metadata": {},
   "source": [
    "## 4. Create or Reset the Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f604e76-1d0f-43b3-95ba-71ff69b204de",
   "metadata": {},
   "source": [
    "## 5. Integrate Components for Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "\n",
    "### RAG Workflow Synergy in Conversational Systems\n",
    "\n",
    "In this step, the workflow demonstrates a synergy between:\n",
    "- **Auto-regressive text generation** for responses.\n",
    "- **Auto-encoding-like embeddings** for retrieval.\n",
    "\n",
    "#### Key Elements Involved\n",
    "1. **LLM Initialization**: An LLM (e.g., GPT-4o-mini) is initialized for generating conversational responses.\n",
    "2. **Memory Setup**: Memory is configured to maintain the context of the conversation.\n",
    "3. **Retriever Configuration**: A retriever is set up to fetch relevant chunks from the vector store during the conversation.\n",
    "4. **Conversational Chain Integration**: All components are combined into a `ConversationalRetrievalChain` to enable retrieval-augmented responses.\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "#### Element 2. Memory Setup\n",
    "The memory in a conversational system serves to:\n",
    "- **Store Conversation History**: Maintain a record of dialogue exchanges so far.\n",
    "- **Provide Context for Responses**: Allow the LLM to generate contextually aware and coherent replies.\n",
    "- **Enhance User Experience**: Enable natural interactions by referencing past messages or decisions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Element 3. Working and Effect of the Retriever\n",
    "\n",
    "Core Functionality:\n",
    "\n",
    "The retriever uses a vector similarity search process to find and return relevant document chunks. The steps are as follows:\n",
    "- **Query Encoding**:\n",
    "  - The retriever receives a query (e.g., user input or LLM-generated question).\n",
    "  - It converts the query into an embedding using the same embedding model used to create the vector store, ensuring compatibility.\n",
    "- **Similarity Search**:\n",
    "  - The vector store contains embeddings for all document chunks stored in high-dimensional space.\n",
    "  - The retriever calculates the cosine similarity (or another metric) between the query embedding and each stored embedding.\n",
    "  - The most relevant matches (highest similarity scores) are identified.\n",
    "- **Document Selection**:\n",
    "  - The retriever fetches the top-N relevant chunks.\n",
    "  - These chunks can be filtered or ranked further, depending on application requirements.\n",
    "\n",
    "#### Element 4. Conversational Chain Integration\n",
    "\n",
    "- The retrieved chunks are passed to the LLM as part of the context for response generation.\n",
    "- The LLM integrates the retrieved information into its auto-regressive generation process.\n",
    "- This process enhances the LLM's ability to provide accurate and context-aware responses, even when the LLM's internal knowledge is limited due to a knowledge cutoff or lack of training data.\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new Chat with OpenAI by initializing the 'ChatOpenAI' object.\n",
    "# Setting the temperature to 0.7 for controlled creativity in responses and specifying the model to use ('MODEL').\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# Setting up conversation memory using 'ConversationBufferMemory' to store chat history.\n",
    "# Using 'memory_key' to name the memory variable and enabling 'return_messages' to retrieve past messages in conversations.\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Setting up the retriever as an abstraction over the VectorStore/a simpler interface to interact with the VectorStore \n",
    "# to simplify the process of querying and retrieving documents from the VectorStore during RAG (Retrieval-Augmented Generation).\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Combining components into a conversation chain using 'ConversationalRetrievalChain'.\n",
    "# The chain integrates the GPT-4o-mini language model (llm), the retriever for document access, and the memory for maintaining chat context.\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968e7bf2-e862-4679-a11f-6c1efb6ec8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurellm is an innovative insurance tech startup founded by Avery Lancaster in 2015, focused on disrupting the insurance industry with its advanced software products. With a workforce of 200 employees and 12 offices across the US, Insurellm offers four main products: Carllm for auto insurance, Homellm for home insurance, Rellm for the reinsurance sector, and Marketllm, a marketplace connecting consumers with insurance providers. The company has rapidly grown to serve over 300 clients worldwide, emphasizing innovation and reliability in the insurance landscape.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you describe Insurellm in a few sentences\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6eb99fb-33ec-4025-ab92-b634ede03647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## 6. Set up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435b2b9-935c-48cd-aaf3-73a837ecde49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
